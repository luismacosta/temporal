---
# Source: temporal/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: temporal/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-grafana
  namespace: default
---
# Source: temporal/charts/prometheus/charts/alertmanager/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-alertmanager
  labels:
    helm.sh/chart: alertmanager-1.11.0
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.27.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
automountServiceAccountToken: true
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.20.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
  name: my-release-kube-state-metrics
  namespace: default
---
# Source: temporal/charts/prometheus/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-release-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.36.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.8.1"
---
# Source: temporal/charts/prometheus/charts/prometheus-pushgateway/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.13.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v1.8.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-prometheus-pushgateway
  namespace: default
automountServiceAccountToken: true
---
# Source: temporal/charts/prometheus/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
  namespace: default
  annotations:
    {}
---
# Source: temporal/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "NXI4Tk5hczZFdm5JWlZ4Sm1MZ3ZMbDFCa3paQ0J1TW96VmFrRGJYbg=="
  ldap-toml: ""
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-temporal-default-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: "dGVtcG9yYWw="
---
# Source: temporal/templates/server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-release-temporal-visibility-store
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
type: Opaque
data:
  password: ""
---
# Source: temporal/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
data:
  
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      isDefault: true
      name: TemporalMetrics
      type: prometheus
      url: http://my-release-prometheus-server
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      editable: true
      folder: ""
      name: default
      options:
        path: /var/lib/grafana/dashboards/default
      orgId: 1
      type: file
  download_dashboards.sh: |
    #!/usr/bin/env sh
    set -euf
    mkdir -p /var/lib/grafana/dashboards/default
  
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/advanced-visibility-specific.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-advanced-visibility-specific-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/clustermonitoring-kubernetes.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-clustermonitoring-kubernetes-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/frontend-service-specific.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-frontend-service-specific-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/history-service-specific.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-history-service-specific-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/matching-service-specific.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-matching-service-specific-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/worker-service-specific.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/misc-worker-service-specific-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/sdk/sdk-general.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/sdk-general-github.json"
      
    curl -skf \
    --connect-timeout 60 \
    --max-time 60 \
    -H "Accept: application/json" \
    -H "Content-Type: application/json;charset=UTF-8" \
      "https://raw.githubusercontent.com/temporalio/dashboards/helm/server/server-general.json" \
      | sed '/-- .* --/! s/"datasource":.*,/"datasource": "TemporalMetrics",/g' \
    > "/var/lib/grafana/dashboards/default/server-general-github.json"
---
# Source: temporal/charts/grafana/templates/dashboards-json-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-grafana-dashboards-default
  namespace: default
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
    dashboard-provider: default
data:
  {}
---
# Source: temporal/charts/prometheus/charts/alertmanager/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-release-alertmanager
  labels:
    helm.sh/chart: alertmanager-1.11.0
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.27.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
data:
  alertmanager.yml: |
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
    templates:
    - /etc/alertmanager/*.tmpl
---
# Source: temporal/charts/prometheus/templates/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
  namespace: default
data:
  allow-snippet-annotations: "false"
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - honor_labels: true
      job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - honor_labels: true
      job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
    - honor_labels: true
      job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
      job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
        replacement: '[$2]:$1'
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: replace
        regex: (\d+);((([0-9]+?)(\.|$)){4})
        replacement: $2:$1
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        - __meta_kubernetes_pod_ip
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: default
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
          regex: my-release
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex: "9093"
          action: keep
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: temporal/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-temporal-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
data:
  config_template.yaml: |-
    log:
      stdout: true
      level: "debug,info"

    persistence:
      defaultStore: default
      visibilityStore: visibility
      numHistoryShards: 512
      datastores:
        default:
          sql:
            pluginName: "postgres12"
            driverName: "postgres12"
            databaseName: "temporal"
            connectAddr: "mysql:3306"
            connectProtocol: "tcp"
            user: temporal
            password: {{ .Env.TEMPORAL_STORE_PASSWORD | quote }}
            maxConnLifetime: 1h
            maxConns: 20
            secretName: ""
        visibility:
          elasticsearch:
            version: "v7"
            url:
                scheme: "http"
                host: "elasticsearch-master-headless:9200"
            username: ""
            password: {{ .Env.TEMPORAL_VISIBILITY_STORE_PASSWORD  | quote }}
            logLevel: "error"
            indices:
                visibility: "temporal_visibility_v1_dev"

    global:
      membership:
        name: temporal
        maxJoinDuration: 30s
        broadcastAddress: {{ default .Env.POD_IP "0.0.0.0" }}

      pprof:
        port: 7936

      metrics:
        tags:
          type: {{ .Env.SERVICES }}
        prometheus:
          timerType: histogram
          listenAddress: "0.0.0.0:9090"

    services:
      frontend:
        rpc:
          grpcPort: 7233
          httpPort: 7243
          membershipPort: 6933
          bindOnIP: "0.0.0.0"

      history:
        rpc:
          grpcPort: 7234
          membershipPort: 6934
          bindOnIP: "0.0.0.0"

      matching:
        rpc:
          grpcPort: 7235
          membershipPort: 6935
          bindOnIP: "0.0.0.0"

      worker:
        rpc:
          membershipPort: 6939
          bindOnIP: "0.0.0.0"

    clusterMetadata:
      enableGlobalDomain: false
      failoverVersionIncrement: 10
      masterClusterName: "active"
      currentClusterName: "active"
      clusterInformation:
        active:
          enabled: true
          initialFailoverVersion: 1
          rpcName: "temporal-frontend"
          rpcAddress: "127.0.0.1:7233"
          httpAddress: "127.0.0.1:7243"

    dcRedirectionPolicy:
      policy: "noop"
      toDC: ""

    archival:
      status: "disabled"
    publicClient:
      hostPort: "my-release-temporal-frontend:7233"

    dynamicConfigClient:
      filepath: "/etc/temporal/dynamic_config/dynamic_config.yaml"
      pollInterval: "10s"
---
# Source: temporal/templates/server-dynamicconfigmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "my-release-temporal-dynamic-config"
  labels:
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
data:
  dynamic_config.yaml: |-
---
# Source: temporal/charts/prometheus/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "8Gi"
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.20.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
  name: my-release-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: temporal/charts/prometheus/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "discovery.k8s.io"
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.20.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
  name: my-release-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: my-release-kube-state-metrics
  namespace: default
---
# Source: temporal/charts/prometheus/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
subjects:
  - kind: ServiceAccount
    name: my-release-prometheus-server
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-release-prometheus-server
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  publishNotReadyAddresses: false
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: temporal/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: temporal/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/charts/alertmanager/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-alertmanager
  labels:
    helm.sh/chart: alertmanager-1.11.0
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.27.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  type: ClusterIP
  ports:
    - port: 9093
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/charts/alertmanager/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-alertmanager-headless
  labels:
    helm.sh/chart: alertmanager-1.11.0
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.27.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  clusterIP: None
  ports:
    - port: 9093
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.20.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.36.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.8.1"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/charts/prometheus-pushgateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
  labels:
    helm.sh/chart: prometheus-pushgateway-2.13.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v1.8.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-prometheus-pushgateway
  namespace: default
spec:
  type: ClusterIP
  ports:
    - port: 9091
      targetPort: 9091
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: my-release
---
# Source: temporal/charts/prometheus/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
  namespace: default
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: temporal/templates/admintools-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-admintools
  labels:
    app.kubernetes.io/component: admintools
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP 
  ports:
    - port: 22
      targetPort: 22
      protocol: TCP
      name: ssh

  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: admintools
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-frontend
  labels:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 7233
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 7243
      targetPort: http
      protocol: TCP
      name: http
      # TODO: Allow customizing the node HTTP port
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-internal-frontend
  labels:
    app.kubernetes.io/component: internal-frontend
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 7236
      targetPort: rpc
      protocol: TCP
      name: grpc-rpc
    - port: 7246
      targetPort: http
      protocol: TCP
      name: http
      # TODO: Allow customizing the node HTTP port
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: internal-frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-frontend-headless
  labels:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-frontend
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  # For Istio service mesh - make sure all ports are defined here and in the deployment:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  # Also for Istio - make sure to set the `appProtocol` property, see:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/#explicit-protocol-selection
  # Note that only the monitoring port is used for discovery (by prometheus).
  # The other ports are listed here solely to allow Istio to configure itself to intercept traffic.
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  ports:
    - port: 7233
      targetPort: rpc
      appProtocol: tcp
      protocol: TCP
      name: grpc-rpc
    - port: 6933
      targetPort: membership
      appProtocol: tcp
      protocol: TCP
      name: grpc-membership
    - port: 9090
      targetPort: metrics
      appProtocol: http
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: frontend
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-matching-headless
  labels:
    app.kubernetes.io/component: matching
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-matching
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  # For Istio service mesh - make sure all ports are defined here and in the deployment:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  # Also for Istio - make sure to set the `appProtocol` property, see:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/#explicit-protocol-selection
  # Note that only the monitoring port is used for discovery (by prometheus).
  # The other ports are listed here solely to allow Istio to configure itself to intercept traffic.
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  ports:
    - port: 7235
      targetPort: rpc
      appProtocol: tcp
      protocol: TCP
      name: grpc-rpc
    - port: 6935
      targetPort: membership
      appProtocol: tcp
      protocol: TCP
      name: grpc-membership
    - port: 9090
      targetPort: metrics
      appProtocol: http
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: matching
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-history-headless
  labels:
    app.kubernetes.io/component: history
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-history
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  # For Istio service mesh - make sure all ports are defined here and in the deployment:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  # Also for Istio - make sure to set the `appProtocol` property, see:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/#explicit-protocol-selection
  # Note that only the monitoring port is used for discovery (by prometheus).
  # The other ports are listed here solely to allow Istio to configure itself to intercept traffic.
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  ports:
    - port: 7234
      targetPort: rpc
      appProtocol: tcp
      protocol: TCP
      name: grpc-rpc
    - port: 6934
      targetPort: membership
      appProtocol: tcp
      protocol: TCP
      name: grpc-membership
    - port: 9090
      targetPort: metrics
      appProtocol: http
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: history
---
# Source: temporal/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-worker-headless
  labels:
    app.kubernetes.io/component: worker
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
    app.kubernetes.io/headless: 'true'
    prometheus.io/job: temporal-worker
    prometheus.io/scrape: 'true'
    prometheus.io/scheme: http
    prometheus.io/port: "9090"

  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  # For Istio service mesh - make sure all ports are defined here and in the deployment:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  # Also for Istio - make sure to set the `appProtocol` property, see:
  # https://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/#explicit-protocol-selection
  # Note that only the monitoring port is used for discovery (by prometheus).
  # The other ports are listed here solely to allow Istio to configure itself to intercept traffic.
  # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
  ports:
    - port: 7239
      targetPort: rpc
      appProtocol: tcp
      protocol: TCP
      name: grpc-rpc
    - port: 6939
      targetPort: membership
      appProtocol: tcp
      protocol: TCP
      name: grpc-membership
    - port: 9090
      targetPort: metrics
      appProtocol: http
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: worker
---
# Source: temporal/templates/web-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-release-temporal-web
  labels:
    app.kubernetes.io/component: web
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: temporal
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/component: web
---
# Source: temporal/charts/prometheus/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-release-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.36.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.8.1"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: my-release
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.36.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.8.1"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: my-release-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.8.1
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: temporal/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.0.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "11.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: my-release
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: my-release
      annotations:
        checksum/config: 7523ecb8ae39e63b34ccf81cd7ebb73a53fbe4dc9193d2d25bcede04dc5af9f1
        checksum/dashboards-json-config: 4ab9f1f35a70dfdf8ef6bbd3aa653e36d58d072a394c033a660fdd6446c05a3d
        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
        checksum/secret: 82017b2c0f8ae7dd2a2451b667232c174f8e5c0bcd9df50aaef235e40ddb25dd
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: my-release-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      initContainers:
        - name: download-dashboards
          image: "docker.io/curlimages/curl:7.85.0"
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh"]
          args: [ "-c", "mkdir -p /var/lib/grafana/dashboards/default && /bin/sh -x /etc/grafana/download_dashboards.sh" ]
          env:
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/download_dashboards.sh"
              subPath: download_dashboards.sh
            - name: storage
              mountPath: "/var/lib/grafana"
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:11.0.0"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
            - name: config
              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
              subPath: "dashboardproviders.yaml"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
      volumes:
        - name: config
          configMap:
            name: my-release-grafana
        - name: dashboards-default
          configMap:
            name: my-release-grafana-dashboards-default
        - name: storage
          emptyDir: {}
---
# Source: temporal/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.20.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "2.12.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: my-release
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.20.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "2.12.0"
    spec:
      hostNetwork: false
      serviceAccountName: my-release-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: temporal/charts/prometheus/charts/prometheus-pushgateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: prometheus-pushgateway-2.13.0
    app.kubernetes.io/name: prometheus-pushgateway
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v1.8.0"
    app.kubernetes.io/managed-by: Helm
  name: my-release-prometheus-pushgateway
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-pushgateway
      app.kubernetes.io/instance: my-release
  template:
    metadata:
      labels:
        helm.sh/chart: prometheus-pushgateway-2.13.0
        app.kubernetes.io/name: prometheus-pushgateway
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "v1.8.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: my-release-prometheus-pushgateway
      automountServiceAccountToken: true
      containers:
        - name: pushgateway
          image: "quay.io/prometheus/pushgateway:v1.8.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: metrics
              containerPort: 9091
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          volumeMounts:
            - name: storage-volume
              mountPath: "/data"
              subPath: ""
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: storage-volume
          emptyDir: {}
---
# Source: temporal/charts/prometheus/templates/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: v2.53.0
    helm.sh/chart: prometheus-25.22.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: prometheus
  name: my-release-prometheus-server
  namespace: default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: my-release
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: prometheus
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: v2.53.0
        helm.sh/chart: prometheus-25.22.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: prometheus
    spec:
      enableServiceLinks: true
      serviceAccountName: my-release-prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --watched-dir=/etc/config
            - --reload-url=http://127.0.0.1:9090/-/reload
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:v2.53.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: my-release-prometheus-server
        - name: storage-volume
          persistentVolumeClaim:
            claimName: my-release-prometheus-server
---
# Source: temporal/templates/admintools-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-admintools
  annotations:
    
  labels:
    app.kubernetes.io/component: admintools
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: admintools
  template:
    metadata:
      annotations:
        
      labels:
        app.kubernetes.io/component: admintools
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      containers:
        - name: admin-tools
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 22
              protocol: TCP
          env:
            # TEMPORAL_CLI_ADDRESS is deprecated, use TEMPORAL_ADDRESS instead
            - name: TEMPORAL_CLI_ADDRESS
              value: my-release-temporal-frontend:7233
            - name: TEMPORAL_ADDRESS
              value: my-release-temporal-frontend:7233
          livenessProbe:
              exec:
                command:
                - ls
                - /
              initialDelaySeconds: 5
              periodSeconds: 5
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-frontend
  annotations:
    
  labels:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: frontend
  template:
    metadata:
      annotations:
        checksum/config: 843f7f3b832425697d2b8059237be1c7330092265fa884617b8cd6a47b87c592
        prometheus.io/job: temporal-frontend
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
        
      labels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
      containers:
        - name: temporal-frontend
          image: "temporalio/server:1.26.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICES
              value: frontend
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
          # For Istio service mesh - make sure ports are defined here and in the headless service, see:
          # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
          ports:
            - name: rpc
              containerPort: 7233
              protocol: TCP
            - name: membership
              containerPort: 6933
              protocol: TCP
            - name: http
              containerPort: 7243
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "my-release-temporal-config"
        - name: dynamic-config
          configMap:
            name: "my-release-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-history
  annotations:
    
  labels:
    app.kubernetes.io/component: history
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: history
  template:
    metadata:
      annotations:
        checksum/config: 843f7f3b832425697d2b8059237be1c7330092265fa884617b8cd6a47b87c592
        prometheus.io/job: temporal-history
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
        
      labels:
        app.kubernetes.io/component: history
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
      containers:
        - name: temporal-history
          image: "temporalio/server:1.26.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICES
              value: history
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
          # For Istio service mesh - make sure ports are defined here and in the headless service, see:
          # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
          ports:
            - name: rpc
              containerPort: 7234
              protocol: TCP
            - name: membership
              containerPort: 6934
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "my-release-temporal-config"
        - name: dynamic-config
          configMap:
            name: "my-release-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-matching
  annotations:
    
  labels:
    app.kubernetes.io/component: matching
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: matching
  template:
    metadata:
      annotations:
        checksum/config: 843f7f3b832425697d2b8059237be1c7330092265fa884617b8cd6a47b87c592
        prometheus.io/job: temporal-matching
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
        
      labels:
        app.kubernetes.io/component: matching
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
      containers:
        - name: temporal-matching
          image: "temporalio/server:1.26.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICES
              value: matching
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
          # For Istio service mesh - make sure ports are defined here and in the headless service, see:
          # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
          ports:
            - name: rpc
              containerPort: 7235
              protocol: TCP
            - name: membership
              containerPort: 6935
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          livenessProbe:
             initialDelaySeconds: 150
             tcpSocket:
               port: rpc
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "my-release-temporal-config"
        - name: dynamic-config
          configMap:
            name: "my-release-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-worker
  annotations:
    
  labels:
    app.kubernetes.io/component: worker
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: worker
  template:
    metadata:
      annotations:
        checksum/config: 843f7f3b832425697d2b8059237be1c7330092265fa884617b8cd6a47b87c592
        prometheus.io/job: temporal-worker
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9090'
        
      labels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      initContainers:
        - name: check-elasticsearch-index
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1 > /dev/null; do echo waiting for elasticsearch index to become ready; sleep 1; done;']
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
      containers:
        - name: temporal-worker
          image: "temporalio/server:1.26.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICES
              value: worker
            - name: TEMPORAL_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
            - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
          # For Istio service mesh - make sure ports are defined here and in the headless service, see:
          # https://istio.io/latest/docs/ops/configuration/traffic-management/traffic-routing/#headless-services
          ports:
            - name: membership
              containerPort: 6939
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          volumeMounts:
            - name: config
              mountPath: /etc/temporal/config/config_template.yaml
              subPath: config_template.yaml
            - name: dynamic-config
              mountPath: /etc/temporal/dynamic_config
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: "my-release-temporal-config"
        - name: dynamic-config
          configMap:
            name: "my-release-temporal-dynamic-config"
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
---
# Source: temporal/templates/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-release-temporal-web
  annotations:
    
  labels:
    app.kubernetes.io/component: web
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: temporal
      app.kubernetes.io/instance: my-release
      app.kubernetes.io/component: web
  template:
    metadata:
      annotations:
        
      labels:
        app.kubernetes.io/component: web
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      containers:
        - name: temporal-web
          image: "temporalio/ui:2.34.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: TEMPORAL_ADDRESS
              value: "my-release-temporal-frontend.default.svc:7233"
          livenessProbe:
            initialDelaySeconds: 10
            tcpSocket:
              port: http
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: temporal/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "my-release"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        release: "my-release"
        chart: "elasticsearch"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                set -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=green&timeout=1s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                  if http "/_cluster/health?wait_for_status=green&timeout=1s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,"
          - name: discovery.seed_hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: cluster.deprecation_indexing.enabled
            value: "false"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: node.ml
            value: "true"
          - name: node.remote_cluster_client
            value: "true"
        volumeMounts:
---
# Source: temporal/charts/prometheus/charts/alertmanager/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-release-alertmanager
  labels:
    helm.sh/chart: alertmanager-1.11.0
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "v0.27.0"
    app.kubernetes.io/managed-by: Helm
  namespace: default
spec:
  replicas: 1
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/instance: my-release
  serviceName: my-release-alertmanager-headless
  template:
    metadata:
      labels:
        app.kubernetes.io/name: alertmanager
        app.kubernetes.io/instance: my-release
      annotations:
        checksum/config: 8ed428d289a1eaa750d0a92a4aa378b3b8306ad574d4dca006441c5714d99139
    spec:
      automountServiceAccountToken: true
      serviceAccountName: my-release-alertmanager
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      containers:
        - name: alertmanager
          securityContext:
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          image: "quay.io/prometheus/alertmanager:v0.27.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --storage.path=/alertmanager
            - --config.file=/etc/alertmanager/alertmanager.yml
          ports:
            - name: http
              containerPort: 9093
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
          volumeMounts:
            - name: config
              mountPath: /etc/alertmanager
            - name: storage
              mountPath: /alertmanager
      volumes:
        - name: config
          configMap:
            name: my-release-alertmanager
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
---
# Source: temporal/templates/server-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-release-temporal-schema-1
  labels:
    app.kubernetes.io/component: database
    app.kubernetes.io/name: temporal
    helm.sh/chart: temporal-0.55.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-release
    app.kubernetes.io/version: "1.26.2"
    app.kubernetes.io/part-of: temporal
spec:
  backoffLimit: 100
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      name: my-release-temporal-schema-1
      labels:
        app.kubernetes.io/component: database
        app.kubernetes.io/name: temporal
        helm.sh/chart: temporal-0.55.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/instance: my-release
        app.kubernetes.io/version: "1.26.2"
        app.kubernetes.io/part-of: temporal
    spec:
      serviceAccountName: default
      restartPolicy: OnFailure
      initContainers:
        - name: check-elasticsearch
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl --silent --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT 2>&1 > /dev/null; do echo waiting for elasticsearch to start; sleep 1; done;']
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
        - name: create-default-store
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['temporal-sql-tool', 'create-database']
          env:
            - name: SQL_PLUGIN
              value: postgres12
            - name: SQL_HOST
              value: mysql
            - name: SQL_PORT
              value: "3306"
            - name: SQL_DATABASE
              value: temporal
            - name: SQL_USER
              value: temporal
            - name: SQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
        - name: setup-default-store
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['temporal-sql-tool', 'setup-schema', '-v', '0.0']
          env:
            - name: SQL_PLUGIN
              value: postgres12
            - name: SQL_HOST
              value: mysql
            - name: SQL_PORT
              value: "3306"
            - name: SQL_DATABASE
              value: temporal
            - name: SQL_USER
              value: temporal
            - name: SQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
        - name: setup-visibility-store
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c']
          args:
            - 'curl -X PUT --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/_template/temporal_visibility_v1_template -H "Content-Type: application/json" --data-binary "@schema/elasticsearch/visibility/index_template_$ES_VERSION.json" 2>&1 &&
              curl --head --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1 ||
              curl -X PUT --fail --user "$ES_USER:$ES_PWD" $ES_SCHEME://$ES_HOST:$ES_PORT/$ES_VISIBILITY_INDEX 2>&1'
          env:
            - name: ES_SCHEME
              value: http
            - name: ES_HOST
              value: elasticsearch-master-headless
            - name: ES_PORT
              value: "9200"
            - name: ES_USER
              value: ""
            - name: ES_PWD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-visibility-store
                  key: password
            - name: ES_VERSION
              value: v7
            - name: ES_VISIBILITY_INDEX
              value: temporal_visibility_v1_dev
        - name: update-default-store
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['temporal-sql-tool', 'update-schema', '--schema-dir', '/etc/temporal/schema/postgresql/v12/temporal/versioned']
          env:
            - name: SQL_PLUGIN
              value: postgres12
            - name: SQL_HOST
              value: mysql
            - name: SQL_PORT
              value: "3306"
            - name: SQL_DATABASE
              value: temporal
            - name: SQL_USER
              value: temporal
            - name: SQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-release-temporal-default-store
                  key: password
      containers:
        - name: done
          image: "temporalio/admin-tools:1.26.2"
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'echo "Store setup completed"']
---
# Source: temporal/templates/server-pdb.yaml
---
---
# Source: temporal/templates/server-pdb.yaml
---
---
# Source: temporal/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-release-fnerk-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "my-release-vybyc-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
